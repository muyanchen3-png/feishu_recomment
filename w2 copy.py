# workflow_system.py

from abc import ABC, abstractmethod
import json
import os
import requests
from bs4 import BeautifulSoup
from typing import Dict, Any, List
from enum import Enum
from dataclasses import dataclass
from datetime import datetime
import threading
import random
import time  # å¿…é¡»æ˜¾å¼å¯¼å…¥


# ================== æŠ½è±¡æ¥å£å®šä¹‰ ==================
class LLM_INTERFACE(ABC):
    @abstractmethod
    def send_message(self, prompt: str, json_flag: bool = False) -> str:
        pass


# ================== èŠ‚ç‚¹é…ç½®ç±» ==================
@dataclass
class NodeConfig:
    node_id: int
    node_type: str
    node_name: str
    input_map: Dict[str, str]
    choice_map: Dict[str, str]
    attrs: Dict[str, Any]


# ================== å·¥ä½œæµèŠ‚ç‚¹åŸºç±» ==================
class WorkflowNode(ABC):
    def __init__(self, config: NodeConfig, workflow_context):
        self.config = config
        self.workflow_context = workflow_context
        self.attrs = config.attrs or {}

    @abstractmethod
    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        pass


# ================== å¾®åšçˆ¬è™«å®ç° ==================
class WeiboCrawler:
    def __init__(self, cookie: str = None):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                          "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://s.weibo.com/weibo?q=%E9%BB%84%E9%87%91%E4%BB%B7%E6%A0%BC",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "same-origin"
        })
        if cookie:
            self.session.headers["Cookie"] = cookie
            print("ğŸª Cookie å·²åŠ è½½")

    def search_posts(self, keyword: str, max_pages: int = 2) -> List[Dict[str, str]]:
        base_url = "https://s.weibo.com/weibo"
        results = []

        for page in range(1, max_pages + 1):
            params = {"q": keyword, "page": page}
            try:
                print(f"ğŸ” æ­£åœ¨è¯·æ±‚ç¬¬ {page} é¡µ: {keyword}")
                response = self.session.get(
                    base_url,
                    params=params,
                    timeout=10,
                    allow_redirects=True
                )

                if response.status_code != 200:
                    print(f"âŒ HTTP {response.status_code}: {response.text[:200]}")
                    continue

                text = response.text

                # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘æˆ–éœ€è¦éªŒè¯
                if "passport.weibo.com" in response.url or "login" in response.url:
                    raise Exception("ç™»å½•å¤±æ•ˆï¼Œè¯·æ›´æ–° Cookie")
                if "éªŒè¯" in text or "è¯·å¼€å¯ JavaScript" in text or "æ£€æŸ¥æµè§ˆå™¨" in text:
                    raise Exception("è§¦å‘åçˆ¬æœºåˆ¶ï¼Œè¯·æ›´æ¢ IP æˆ–ä½¿ç”¨ä»£ç†")

                soup = BeautifulSoup(text, "lxml")

                cards = soup.find_all("div", class_="card-wrap")
                extracted = 0

                for card in cards:
                    # è¿‡æ»¤éç”¨æˆ·å¾®åšï¼ˆå¦‚çƒ­æœæ¦œã€å¹¿å‘Šï¼‰
                    script_tag = card.find("script")
                    if script_tag and "hotsearch" in script_tag.text:
                        continue

                    user_elem = card.find("a", class_="name")
                    username = user_elem.get_text(strip=True) if user_elem else "æœªçŸ¥ç”¨æˆ·"

                    content_elem = card.find("p", class_=lambda x: x and "txt" in x)
                    if not content_elem:
                        continue
                    content = content_elem.get_text(strip=True).replace("æ”¶èµ·å…¨æ–‡", "").strip()
                    if len(content) < 5:
                        continue

                    date_elem = card.find("p", class_="from")
                    date = "æœªçŸ¥æ—¶é—´"
                    url = ""
                    if date_elem and date_elem.find("a"):
                        date = date_elem.find("a").get_text(strip=True)
                        href = date_elem.find("a").get("href")
                        url = f"https://s.weibo.com{href}" if href.startswith("/") else href

                    results.append({
                        "username": username,
                        "text": content,
                        "date": date,
                        "source": "weibo",
                        "url": url
                    })
                    extracted += 1

                print(f"âœ… ç¬¬ {page} é¡µæå–åˆ° {extracted} æ¡å¾®åš")
                time.sleep(random.uniform(2.5, 4.0))  # æ§åˆ¶é¢‘ç‡

            except Exception as e:
                print(f"ğŸš¨ çˆ¬å–å¤±è´¥: {e}")
                break  # å‡ºé”™å³åœæ­¢

        return results[:30]  # è¿”å›æœ€å¤š30æ¡


# ================== å…·ä½“èŠ‚ç‚¹å®ç° ==================
class ReceiveInputNode(WorkflowNode):
    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        print(f"ğŸ“¥ æ¥æ”¶åˆ°è¾“å…¥å…³é”®è¯: {input_data['keyword']}")
        return {"keyword": input_data["keyword"], "timestamp": datetime.now().isoformat()}


class WeiboCrawlNode(WorkflowNode):
    def __init__(self, config: NodeConfig, workflow_context):
        super().__init__(config, workflow_context)
        # ä¼˜å…ˆä½¿ç”¨èŠ‚ç‚¹å±æ€§ä¸­çš„ cookieï¼Œå¦åˆ™ç”¨ç¯å¢ƒå˜é‡æˆ–å…¨å±€ç¡¬ç¼–ç 
        self.cookie = self.attrs.get("cookie") or os.getenv("WEIBO_COOKIE") or WEIBO_COOKIE
        if not self.cookie:
            raise ValueError("âš ï¸ é”™è¯¯ï¼šæœªæä¾› WEIBO_COOKIEï¼Œæ— æ³•çˆ¬å–å¾®åšæ•°æ®")
        self.crawler = WeiboCrawler(cookie=self.cookie)

    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        keyword = input_data.get("keyword")
        if not keyword:
            return {"success": False, "posts": [], "error": "ç¼ºå°‘å…³é”®è¯"}

        print(f"ğŸ•·ï¸ å¼€å§‹çˆ¬å–å¾®åšæ•°æ®: {keyword}")
        posts = self.crawler.search_posts(keyword, max_pages=2)

        if not posts:
            return {"success": False, "posts": [], "error": f"æœªæ‰¾åˆ°å…³äº '{keyword}' çš„å¾®åšä¿¡æ¯"}

        print(f"ğŸ“Œ æˆåŠŸè·å– {len(posts)} æ¡å¾®åš")
        return {"success": True, "posts": posts}


class LLMSummarizeNode(WorkflowNode):
    def __init__(self, config: NodeConfig, workflow_context):
        super().__init__(config, workflow_context)
        self.llm_client = workflow_context["llm_client"]

    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        posts = input_data.get("posts", [])
        if not posts:
            return {"summary": "æ— å¯ç”¨å†…å®¹è¿›è¡Œæ€»ç»“"}

        texts = "\n".join([f"{p['username']}: {p['text']}" for p in posts])
        prompt_file = "./prompt/summarize_weibo.md"
        os.makedirs("./prompt", exist_ok=True)

        if not os.path.exists(prompt_file):
            with open(prompt_file, "w", encoding="utf-8") as f:
                f.write("""ä½ æ˜¯ä¸€ä¸ªèˆ†æƒ…åˆ†æä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å¾®åšå†…å®¹ï¼Œæ€»ç»“å…¬ä¼—å¯¹ã€{{keyword}}ã€‘çš„çœ‹æ³•ã€‚
è¦æ±‚ï¼š
1. åˆ†ç‚¹åˆ—å‡ºä¸»è¦è§‚ç‚¹ï¼ˆè‡³å°‘3ç‚¹ï¼‰
2. æ¯ä¸ªè§‚ç‚¹é™„ä¸Šä»£è¡¨æ€§åŸå¥ï¼ˆå¼•ç”¨ç”¨æˆ·å+å†…å®¹ï¼‰
3. æ€»ç»“æ•´ä½“æƒ…ç»ªå€¾å‘ï¼ˆä¹è§‚/æ‚²è§‚/ä¸­ç«‹ï¼‰
4. ä½¿ç”¨ä¸­æ–‡è¾“å‡º

å¾®åšå†…å®¹ï¼š
{{content}}

è¯·æŒ‰ä¸Šè¿°æ ¼å¼å›ç­”ã€‚
""")
            print(f"âœ… æç¤ºè¯æ–‡ä»¶åˆ›å»º: {prompt_file}")

        with open(prompt_file, "r", encoding="utf-8") as f:
            prompt_template = f.read()

        final_prompt = prompt_template \
            .replace("{{keyword}}", input_data.get('keyword', 'æœªçŸ¥è¯é¢˜')) \
            .replace("{{content}}", texts[:8000])  # æˆªæ–­é˜²æ­¢è¶…é™

        print("ğŸ§  æ­£åœ¨è°ƒç”¨ LLM ç”Ÿæˆæ‘˜è¦...")
        summary = self.llm_client.send_message(final_prompt)
        return {"summary": summary}


class FeishuNotifyNode(WorkflowNode):
    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        summary = input_data.get("summary", "æ— æ‘˜è¦å†…å®¹")
        webhook_url = self.attrs.get("webhook") or os.getenv("FEISHU_WEBHOOK")
        if not webhook_url:
            print("âŒ æœªé…ç½®é£ä¹¦ Webhookï¼Œè·³è¿‡é€šçŸ¥")
            return {"notified": False}

        msg = {
            "msg_type": "text",
            "content": {"text": f"ã€ä»Šæ—¥èˆ†æƒ…ç®€æŠ¥ã€‘\n\n{summary}\n\n---\nğŸ¤– è‡ªåŠ¨ç”Ÿæˆäº {datetime.now().strftime('%Y-%m-%d %H:%M')}"}
        }
        try:
            res = requests.post(webhook_url, json=msg, timeout=5)
            if res.status_code == 200:
                print("âœ… å·²æ¨é€åˆ°é£ä¹¦")
                return {"notified": True}
            else:
                print(f"âŒ æ¨é€å¤±è´¥: {res.text}")
                return {"notified": False}
        except Exception as e:
            print(f"âŒ æ¨é€å¼‚å¸¸: {e}")
            return {"notified": False}


# ================== å·¥ä½œæµç®¡ç†å™¨ ==================
class WorkflowManager:
    def __init__(self, api_key: str, prompt_folder: str, memory_path: str):
        self.api_key = api_key
        self.prompt_folder = prompt_folder
        self.memory_path = memory_path
        self.nodes = {}
        self.workflow_context = {
            "llm_client": SimpleLLMClient(api_key),
            "memory": self.load_memory()
        }

    def register_node(self, config: NodeConfig):
        node_type = config.node_type
        if node_type == "receive_input":
            node = ReceiveInputNode(config, self.workflow_context)
        elif node_type == "weibo_crawl":
            node = WeiboCrawlNode(config, self.workflow_context)
        elif node_type == "llm_summarize":
            node = LLMSummarizeNode(config, self.workflow_context)
        elif node_type == "feishu_notify":
            node = FeishuNotifyNode(config, self.workflow_context)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„èŠ‚ç‚¹ç±»å‹: {node_type}")
        self.nodes[config.node_name] = node

    def run_workflow(self, inputs: Dict[str, Any], flow_config: List[Dict]):
        data_pool = {"input": inputs}
        last_output = None

        for step in flow_config:
            node_name = step["node_name"]
            node = self.nodes[node_name]

            # æ„å»ºè¾“å…¥
            input_data = {}
            for key, src in step["input_map"].items():
                source_node, output_key = src.split(".")
                input_data[key] = data_pool[source_node][output_key]

            # æ‰§è¡ŒèŠ‚ç‚¹
            print(f"\nğŸš€ æ‰§è¡ŒèŠ‚ç‚¹: {node_name}")
            try:
                output = node.execute(input_data)
                data_pool[node_name] = output
                last_output = output
            except Exception as e:
                print(f"âŒ æ‰§è¡Œå¤±è´¥: èŠ‚ç‚¹ {node_name} æ‰§è¡Œå¤±è´¥: {e}")
                return None

        return last_output

    def load_memory(self) -> Dict[str, Any]:
        if os.path.exists(self.memory_path):
            with open(self.memory_path, "r", encoding="utf-8") as f:
                return json.load(f)
        return {}


# ================== ç®€æ˜“ LLM å®¢æˆ·ç«¯ï¼ˆé€šä¹‰åƒé—®ï¼‰==================
class SimpleLLMClient:
    def __init__(self, api_key: str):
        self.api_key = api_key
        # âœ… ä½¿ç”¨é˜¿é‡Œäº‘ DashScope çš„ OpenAI å…¼å®¹æ¥å£
        self.endpoint = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
        self.model = "deepseek-r1"  # ä¹Ÿæ”¯æŒ qwen-plus, qwen-turbo ç­‰

    def send_message(self, prompt: str, json_flag: bool = False) -> str:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "Accept": "application/json"
        }

        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.7,
        }

        if json_flag:
            # å¯ç”¨ JSON è¾“å‡ºæ¨¡å¼ï¼ˆéœ€æ¨¡å‹æ”¯æŒï¼‰
            payload["response_format"] = {"type": "json_object"}

        try:
            print("ğŸ§  æ­£åœ¨è¯·æ±‚é˜¿é‡Œäº‘ Qwen...")  # è°ƒè¯•ä¿¡æ¯
            response = requests.post(
                self.endpoint,
                json=payload,
                headers=headers,
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                print("âœ… LLM è¿”å›æˆåŠŸ")
                return content
            else:
                error_msg = response.text
                print(f"âŒ LLM è¯·æ±‚å¤±è´¥: {response.status_code} {error_msg}")
                return f"[LLM é”™è¯¯] {response.status_code}: {error_msg}"

        except Exception as e:
            print(f"ğŸš¨ LLM è¯·æ±‚å¼‚å¸¸: {str(e)}")
            return f"[è¯·æ±‚å¼‚å¸¸] {str(e)}"


# ================== FastAPI æ¥å£ ==================
from fastapi import FastAPI, HTTPException, Form
from pydantic import BaseModel
import threading

from contextlib import asynccontextmanager
from fastapi.middleware.cors import CORSMiddleware
@asynccontextmanager
async def lifespan(app: FastAPI):
    print("ğŸŒ å¯åŠ¨ä¸­ï¼šåˆå§‹åŒ– WorkflowManager...")
    init_workflow()  # è°ƒç”¨ä½ çš„åˆå§‹åŒ–å‡½æ•°
    yield
    print("ğŸ‘‹ å…³é—­ä¸­ï¼šé‡Šæ”¾èµ„æº...")

app = FastAPI(
    title="èˆ†æƒ…åˆ†æå·¥ä½œæµ API",
    version="1.0",
    lifespan=lifespan  # âœ… ä½¿ç”¨æ–°çš„ lifespan æœºåˆ¶
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # âœ… å…è®¸æ‰€æœ‰æ¥æºï¼ˆå¼€å‘ç”¨ï¼‰â€”â€”ç”Ÿäº§ç¯å¢ƒå»ºè®®æŒ‡å®šå…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],  # å…è®¸æ‰€æœ‰æ–¹æ³•ï¼ˆGET, POST, OPTIONS ç­‰ï¼‰
    allow_headers=["*"],   # å…è®¸æ‰€æœ‰å¤´éƒ¨
)
# å…¨å±€å…±äº«çš„ WorkflowManager å®ä¾‹
manager = None

# è¯·æ±‚å‚æ•°æ¨¡å‹
class AnalysisRequest(BaseModel):
    keyword: str

# åˆå§‹åŒ– managerï¼ˆåœ¨å¯åŠ¨æ—¶åŠ è½½ï¼‰
def init_workflow():
    global manager

    # ========== âš ï¸ è¯·æ›¿æ¢ä¸ºä½ çš„å¯†é’¥å’Œ webhook ==========
    FEISHU_WEBHOOK = "https://open.feishu.cn/open-apis/bot/v2/hook/77bf45ee-a658-4476-ac7e-cf3e9f538fae"
    WEIBO_COOKIE = "SUB=_2A25EJG3wDeRhGeFJ7VMQ9ynOyD-IHXVnWO84rDV8PUNbmtANLWzhkW9Nf5Rur3Ely273qG6tp5U59H4NeXMgOrnH; SCF=AtJxDRAJhb0kFq4S0x0diFZ5wYp67yN-uAqb1OC2du4DgBjXQQf5o869ffqG2C8uto3ZNXMRCGjcLiJvxuCTYLw.; WBPSESS=VIMat820zjL5rTEoO9y5yaz5tE2jZiw_cZ_9IKkZORe_LfwVc3H8l7N9TsRHy5QGfrpKte-wk55a90vPZ6tOVlILz-2lh5cbarozR32-C1u0ESkz7TeApagK61mEbOCVGRNPGQvDZPgHrDcD765DBA=="
    os.environ["WEIBO_COOKIE"] = WEIBO_COOKIE
    if FEISHU_WEBHOOK:
        os.environ["FEISHU_WEBHOOK"] = FEISHU_WEBHOOK

    # åˆ›å»ºç›®å½•
    os.makedirs("./prompt", exist_ok=True)
    os.makedirs("./workflow_memory", exist_ok=True)

    # åˆå§‹åŒ–ç®¡ç†å™¨
    manager = WorkflowManager(
        api_key="sk-ä½ çš„é˜¿é‡Œäº‘DashScope-Key",  # âœ… æ›¿æ¢ä¸ºä½ çš„çœŸå® Key
        prompt_folder="./prompt",
        memory_path="./workflow_memory/memory.json"
    )

    # æ³¨å†ŒèŠ‚ç‚¹ï¼ˆåªæ³¨å†Œä¸€æ¬¡ï¼‰
    nodes_config = [
        {
            "node_id": 1,
            "node_type": "receive_input",
            "node_name": "receive_01",
            "input_map": {},
            "choice_map": {"default": "weibo_crawl_01"},
            "attrs": {}
        },
        {
            "node_id": 2,
            "node_type": "weibo_crawl",
            "node_name": "weibo_crawl_01",
            "input_map": {"keyword": "receive_01.keyword"},
            "choice_map": {"default": "llm_summarize"},
            "attrs": {"cookie": WEIBO_COOKIE}
        },
        {
            "node_id": 3,
            "node_type": "llm_summarize",
            "node_name": "llm_summarize",
            "input_map": {"posts": "weibo_crawl_01.posts", "keyword": "receive_01.keyword"},
            "choice_map": {"default": "feishu_notify"},
            "attrs": {}
        },
        {
            "node_id": 4,
            "node_type": "feishu_notify",
            "node_name": "feishu_notify",
            "input_map": {"summary": "llm_summarize.summary"},
            "choice_map": {},
            "attrs": {"webhook": FEISHU_WEBHOOK}
        }
    ]

    for cfg in nodes_config:
        config = NodeConfig(**cfg)
        manager.register_node(config)

    print("âœ… å·¥ä½œæµç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")


# å‰ç«¯æäº¤åˆ†æè¯·æ±‚
@app.post("/analyze", summary="å¯åŠ¨èˆ†æƒ…åˆ†æ", description="æ¥æ”¶å‰ç«¯ä¼ æ¥çš„å…³é”®è¯ï¼Œæ‰§è¡Œå¾®åšçˆ¬å– + LLM åˆ†æ")
async def start_analysis(request: AnalysisRequest):
    if not manager:
        raise HTTPException(status_code=500, detail="å·¥ä½œæµæœªåˆå§‹åŒ–")

    print(f"ğŸŒ æ”¶åˆ°æ¥è‡ªå‰ç«¯çš„è¯·æ±‚ï¼šå…³é”®è¯ = {request.keyword}")

    flow = [
        {"node_name": "receive_01", "input_map": {"keyword": "input.keyword"}},
        {"node_name": "weibo_crawl_01", "input_map": {"keyword": "receive_01.keyword"}},
        {"node_name": "llm_summarize", "input_map": {"posts": "weibo_crawl_01.posts", "keyword": "receive_01.keyword"}},
        {"node_name": "feishu_notify", "input_map": {"summary": "llm_summarize.summary"}}
    ]

    try:
        result = manager.run_workflow({"keyword": request.keyword}, flow)
        if result and "summary" in result:
            return {
                "success": True,
                "keyword": request.keyword,
                "summary": result["summary"],
                "timestamp": datetime.now().isoformat()
            }
        else:
            return {
                "success": False,
                "error": "åˆ†æå¤±è´¥ï¼Œæ— ç»“æœ"
            }
    except Exception as e:
        return {"success": False, "error": str(e)}

# å¥åº·æ£€æŸ¥æ¥å£
@app.get("/health", summary="å¥åº·æ£€æŸ¥")
async def health_check():
    return {"status": "ok", "time": datetime.now().isoformat()}