# workflow_system.py

from abc import ABC, abstractmethod
import json
import os
import requests
from bs4 import BeautifulSoup
from typing import Dict, Any, List
from enum import Enum
from dataclasses import dataclass
from datetime import datetime
import threading
import random
import time  # å¿…é¡»æ˜¾å¼å¯¼å…¥
# åœ¨é¡¶éƒ¨æ·»åŠ 
from weibo_login import WeiboCookieFetcher

class LLM_INTERFACE(ABC):
    @abstractmethod
    def send_message(self, prompt: str, json_flag: bool = False) -> str:
        pass

@dataclass
class NodeConfig:
    node_id: int
    node_type: str
    node_name: str
    input_map: Dict[str, str]
    choice_map: Dict[str, str]
    attrs: Dict[str, Any]


class WorkflowNode(ABC):
    def __init__(self, config: NodeConfig, workflow_context):
        self.config = config
        self.workflow_context = workflow_context
        self.attrs = config.attrs or {}

    @abstractmethod
    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        pass

class WeiboCrawler:
    def __init__(self, cookie: str = None):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                          "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://s.weibo.com/weibo?q=%E9%BB%84%E9%87%91%E4%BB%B7%E6%A0%BC",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "same-origin"
        })
        if cookie:
            self.session.headers["Cookie"] = cookie
            print("ğŸª Cookie å·²åŠ è½½")

    def search_posts(self, keyword: str, max_pages: int = 2) -> List[Dict[str, str]]:
        base_url = "https://s.weibo.com/weibo"
        results = []

        for page in range(1, max_pages + 1):
            params = {"q": keyword, "page": page}
            try:
                print(f"ğŸ” æ­£åœ¨è¯·æ±‚ç¬¬ {page} é¡µ: {keyword}")
                response = self.session.get(
                    base_url,
                    params=params,
                    timeout=10,
                    allow_redirects=True
                )

                if response.status_code != 200:
                    print(f"âŒ HTTP {response.status_code}: {response.text[:200]}")
                    continue

                text = response.text

                # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘æˆ–éœ€è¦éªŒè¯
                if "passport.weibo.com" in response.url or "login" in response.url:
                    raise Exception("ç™»å½•å¤±æ•ˆï¼Œè¯·æ›´æ–° Cookie")
                if "éªŒè¯" in text or "è¯·å¼€å¯ JavaScript" in text or "æ£€æŸ¥æµè§ˆå™¨" in text:
                    raise Exception("è§¦å‘åçˆ¬æœºåˆ¶ï¼Œè¯·æ›´æ¢ IP æˆ–ä½¿ç”¨ä»£ç†")

                soup = BeautifulSoup(text, "lxml")

                cards = soup.find_all("div", class_="card-wrap")
                extracted = 0

                for card in cards:
                    # è¿‡æ»¤éç”¨æˆ·å¾®åšï¼ˆå¦‚çƒ­æœæ¦œã€å¹¿å‘Šï¼‰
                    script_tag = card.find("script")
                    if script_tag and "hotsearch" in script_tag.text:
                        continue

                    user_elem = card.find("a", class_="name")
                    username = user_elem.get_text(strip=True) if user_elem else "æœªçŸ¥ç”¨æˆ·"

                    content_elem = card.find("p", class_=lambda x: x and "txt" in x)
                    if not content_elem:
                        continue
                    content = content_elem.get_text(strip=True).replace("æ”¶èµ·å…¨æ–‡", "").strip()
                    if len(content) < 5:
                        continue

                    date_elem = card.find("p", class_="from")
                    date = "æœªçŸ¥æ—¶é—´"
                    url = ""
                    if date_elem and date_elem.find("a"):
                        date = date_elem.find("a").get_text(strip=True)
                        href = date_elem.find("a").get("href")
                        url = f"https://s.weibo.com{href}" if href.startswith("/") else href

                    results.append({
                        "username": username,
                        "text": content,
                        "date": date,
                        "source": "weibo",
                        "url": url
                    })
                    extracted += 1

                print(f"âœ… ç¬¬ {page} é¡µæå–åˆ° {extracted} æ¡å¾®åš")
                time.sleep(random.uniform(2.5, 4.0))  # æ§åˆ¶é¢‘ç‡

            except Exception as e:
                print(f"ğŸš¨ çˆ¬å–å¤±è´¥: {e}")
                break  # å‡ºé”™å³åœæ­¢

        return results[:30]  # è¿”å›æœ€å¤š30æ¡



class ReceiveInputNode(WorkflowNode):
    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        print(f"ğŸ“¥ æ¥æ”¶åˆ°è¾“å…¥å…³é”®è¯: {input_data['keyword']}")
        return {"keyword": input_data["keyword"], "timestamp": datetime.now().isoformat()}


class WeiboCrawlNode(WorkflowNode):
    def __init__(self, config: NodeConfig, workflow_context):
        super().__init__(config, workflow_context)
        # ä¼˜å…ˆä½¿ç”¨èŠ‚ç‚¹å±æ€§ä¸­çš„ cookieï¼Œå¦åˆ™ç”¨ç¯å¢ƒå˜é‡æˆ–å…¨å±€ç¡¬ç¼–ç 
        self.cookie = self.attrs.get("cookie") or os.getenv("WEIBO_COOKIE") or WEIBO_COOKIE
        if not self.cookie:
            raise ValueError("âš ï¸ é”™è¯¯ï¼šæœªæä¾› WEIBO_COOKIEï¼Œæ— æ³•çˆ¬å–å¾®åšæ•°æ®")
        self.crawler = WeiboCrawler(cookie=self.cookie)

    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        keyword = input_data.get("keyword")
        if not keyword:
            return {"success": False, "posts": [], "error": "ç¼ºå°‘å…³é”®è¯"}

        # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ¨é€ï¼Œå¦‚æœæ˜¯åˆ™çˆ¬å–æ›´å¤šé¡µæ•°ï¼Œæ”¶é›†æ›´å¤šæ•°æ®
        push_time = input_data.get("push_time", "normal")
        if push_time == "test_10s":
            max_pages = 5  # æµ‹è¯•æ—¶çˆ¬å–5é¡µï¼Œæ”¶é›†æ›´å¤šæ•°æ®
            print(f"ğŸ”¥ æµ‹è¯•æ¨¡å¼: å¼€å§‹æ·±åº¦çˆ¬å–å¾®åšæ•°æ® (5é¡µ): {keyword}")
        else:
            max_pages = 2  # æ­£å¸¸æ¨¡å¼2é¡µ
            print(f"ğŸ•·ï¸ å¼€å§‹çˆ¬å–å¾®åšæ•°æ®: {keyword}")

        posts = self.crawler.search_posts(keyword, max_pages=max_pages)

        if not posts:
            return {"success": False, "posts": [], "error": f"æœªæ‰¾åˆ°å…³äº '{keyword}' çš„å¾®åšä¿¡æ¯"}

        print(f"ğŸ“Œ æˆåŠŸè·å– {len(posts)} æ¡å¾®åš")

        # å°†å¾®åšæ•°æ®ä¿å­˜ä¸ºJSONæ–‡ä»¶
        import json
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_filename = f"./weibo_data_{keyword}_{timestamp}.json"

        try:
            with open(json_filename, 'w', encoding='utf-8') as f:
                json.dump(posts, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ å¾®åšæ•°æ®å·²ä¿å­˜åˆ°: {json_filename}")
        except Exception as e:
            print(f"ğŸš¨ ä¿å­˜å¾®åšæ•°æ®å¤±è´¥: {e}")

        return {"success": True, "posts": posts, "data_file": json_filename}


class LLMSummarizeNode(WorkflowNode):
    def __init__(self, config: NodeConfig, workflow_context):
        super().__init__(config, workflow_context)
        self.llm_client = workflow_context["llm_client"]
        self.topic_memory = workflow_context.get("topic_memory")  # è·å–è¯é¢˜è®°å¿†ç®¡ç†å™¨

    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        posts = input_data.get("posts", [])
        keyword = input_data.get("keyword", "æœªçŸ¥è¯é¢˜")

        if not posts:
            return {"summary": "æ— å¯ç”¨å†…å®¹è¿›è¡Œæ€»ç»“"}

        texts = "\n".join([f"{p['username']}: {p['text']}" for p in posts])

        # è·å–å†å²è®°å¿†
        history_summaries = []
        if self.topic_memory:
            history_summaries = self.topic_memory.get_topic_history(keyword, limit=3)
            print(f"ğŸ§  æ‰¾åˆ° {len(history_summaries)} æ¡å†å²è®°å½•ä½œä¸ºåˆ†æå‚è€ƒ")

        # æ„å»ºå†å²è®°å¿†ä¸Šä¸‹æ–‡
        history_context = ""
        if history_summaries:
            history_list = "\n".join([f"â€¢ {summary}" for summary in history_summaries])
            history_context = f"\n\nğŸ“š å†å²åˆ†æè®°å½•ï¼ˆæœ€è¿‘3æ¬¡ï¼‰ï¼š\n{history_list}"

        prompt_file = "./prompt/summarize_weibo.md"
        os.makedirs("./prompt", exist_ok=True)

        if not os.path.exists(prompt_file):
            with open(prompt_file, "w", encoding="utf-8") as f:
                f.write("""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¤¾äº¤åª’ä½“åˆ†æä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹ä»å¾®åšè·å–çš„çœŸå®ç”¨æˆ·å‘è¨€ï¼Œå¯¹"{{ keyword }}"è¯é¢˜è¿›è¡Œæ·±å…¥åˆ†æã€‚

## åˆ†æè¦æ±‚ï¼š
1. **å†…å®¹å®šä½**ï¼šåŸºäºå‘è¨€å†…å®¹è‡ªåŠ¨åˆ¤æ–­è¿™æ˜¯ï¼ˆé‡‘è/æŠ•èµ„è¯é¢˜ï¼‰è¿˜æ˜¯ï¼ˆæ–°é—»äº‹ä»¶/ç¤¾ä¼šçƒ­ç‚¹ï¼‰ï¼Œé€‰æ‹©åˆé€‚çš„åˆ†ææ¡†æ¶

2. **æ™ºèƒ½åˆ†æ**ï¼š
   - **é‡‘èé¢†åŸŸ**ï¼ˆå¦‚è‚¡ç¥¨ã€é»„é‡‘ã€å¤–æ±‡ã€åŸºé‡‘ç­‰ï¼‰ï¼šé‡ç‚¹åˆ†æèˆ†è®ºå¯¹ä»·æ ¼èµ°åŠ¿çš„å½±å“ã€å¸‚åœºæƒ…ç»ªé¢„æµ‹ã€æŠ•èµ„å»ºè®®å€¾å‘
   - **æ–°é—»äº‹ä»¶**ï¼ˆå¦‚ç«ç¾ã€äº‹æ•…ã€æ”¿ç­–ã€ç¤¾ä¼šäº‹ä»¶ç­‰ï¼‰ï¼šé‡ç‚¹æ¢³ç†äº‹ä»¶è„‰ç»œã€ç›®å‰å‘å±•çŠ¶å†µã€å…¬ä¼—å…³æ³¨ç„¦ç‚¹

3. **è¾“å‡ºç»“æ„**ï¼š
   - ğŸ“Š **äº‹ä»¶æ¦‚è¿°**ï¼šå‘ç”Ÿäº†ä»€ä¹ˆï¼Œç›®å‰çŠ¶æ€å¦‚ä½•
   - ğŸ’­ **èˆ†è®ºæ€åº¦**ï¼šå¤§å®¶çš„æ€åº¦æ˜¯ä¹è§‚/æ‚²è§‚ï¼Œä¸­æ€§å ä¸»å¯¼å—
   - ğŸ”¥ **å…³é”®çƒ­ç‚¹**ï¼šæœ€å—å…³æ³¨çš„å‡ ä¸ªè®¨è®ºç‚¹
   - ğŸ“ˆ **è¶‹åŠ¿å±•æœ›**ï¼ˆé‡‘èè¯é¢˜ï¼‰ï¼šå¸‚åœºæˆ–ä»·æ ¼å¯èƒ½ä¼šå¦‚ä½•å‘å±•

4. **å‘ˆç°æ–¹å¼**ï¼šæ¸…æ–°æ˜“æ‡‚ï¼Œç®€æ´ä¸å•°å—¦ï¼Œä¸è¶…è¿‡400å­—

**å¾®åšå‘è¨€æ•°æ®**ï¼š
{% for post in weibo_posts %}
- [{{ post.username }} @ {{ post.date }}]ï¼š{{ post.text }}
{% endfor %}
""")
            print(f"âœ… æç¤ºè¯æ–‡ä»¶åˆ›å»º: {prompt_file}")

        with open(prompt_file, "r", encoding="utf-8") as f:
            prompt_template = f.read()

        final_prompt = prompt_template \
            .replace("{{ keyword }}", keyword) \
            .replace("{% for post in weibo_posts %}", "") \
            .replace("{% endfor %}", "") \
            .replace("{{content}}", texts[:7500] + history_context)  # ç»„åˆå¾®åšå†…å®¹å’Œå†å²è®°å¿†

        print("ğŸ§  æ­£åœ¨è°ƒç”¨ LLM ç”Ÿæˆæ‘˜è¦...")
        summary = self.llm_client.send_message(final_prompt)

        # å°†æ–°çš„æ€»ç»“å­˜å‚¨åˆ°è®°å¿†ä¸­
        if self.topic_memory and summary and len(summary.strip()) > 20:
            self.topic_memory.add_topic_summary(keyword, summary)
            print(f"ğŸ’¾ æ–°æ€»ç»“å·²å­˜å‚¨åˆ°è¯é¢˜è®°å¿†: {keyword}")

        return {"summary": summary}


class FeishuNotifyNode(WorkflowNode):
    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        summary = input_data.get("summary", "æ— æ‘˜è¦å†…å®¹")
        webhook_url = self.attrs.get("webhook") or os.getenv("FEISHU_WEBHOOK")
        if not webhook_url:
            print("âŒ æœªé…ç½®é£ä¹¦ Webhookï¼Œè·³è¿‡é€šçŸ¥")
            return {"notified": False}

        msg = {
            "msg_type": "text",
            "content": {"text": f"ã€ä»Šæ—¥å…³æ³¨ç‚¹ç®€æŠ¥ã€‘\n\n{summary}\n\n---\nğŸ¤– è‡ªåŠ¨ç”Ÿæˆäº {datetime.now().strftime('%Y-%m-%d %H:%M')}"}
        }
        try:
            res = requests.post(webhook_url, json=msg, timeout=5)
            if res.status_code == 200:
                print("âœ… å·²æ¨é€åˆ°é£ä¹¦")
                return {"notified": True}
            else:
                print(f"âŒ æ¨é€å¤±è´¥: {res.text}")
                return {"notified": False}
        except Exception as e:
            print(f"âŒ æ¨é€å¼‚å¸¸: {e}")
            return {"notified": False}


# ================== æ¯æ—¥æ•°æ®ç´¯ç§¯å™¨ ==================
class DailyDataCollector:
    def __init__(self, data_dir="./daily_data"):
        self.data_dir = data_dir
        os.makedirs(data_dir, exist_ok=True)

    def get_daily_file(self, date_str: str) -> str:
        """è·å–å½“å¤©çš„ç´¯ç§¯æ•°æ®æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self.data_dir, f"daily_{date_str}.json")

    def load_daily_data(self, date_str: str) -> Dict[str, Any]:
        """åŠ è½½å½“å¤©çš„ç´¯ç§¯æ•°æ®"""
        file_path = self.get_daily_file(date_str)
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}

    def save_daily_data(self, date_str: str, data: Dict[str, Any]):
        """ä¿å­˜å½“å¤©çš„ç´¯ç§¯æ•°æ®"""
        file_path = self.get_daily_file(date_str)
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ä¿å­˜æ¯æ—¥æ•°æ®å¤±è´¥: {e}")

    def append_posts(self, date_str: str, keyword: str, posts: List[Dict]):
        """å‘å½“å¤©æ•°æ®ç´¯ç§¯ä¸­æ·»åŠ æ–°çˆ¬å–çš„å¸–å­"""
        daily_data = self.load_daily_data(date_str)

        if keyword not in daily_data:
            daily_data[keyword] = {"posts": [], "total_count": 0, "timestamps": []}

        # æ£€æŸ¥æ˜¯å¦é‡å¤ï¼ˆç›¸åŒURLï¼‰
        existing_urls = {post['url'] for post in daily_data[keyword]["posts"]}

        new_posts = []
        for post in posts:
            if post['url'] not in existing_urls:
                new_posts.append({
                    **post,
                    "collected_at": datetime.now().isoformat(),
                    "batch_hour": datetime.now().hour
                })

        daily_data[keyword]["posts"].extend(new_posts)
        daily_data[keyword]["total_count"] = len(daily_data[keyword]["posts"])
        daily_data[keyword]["timestamps"].append(datetime.now().isoformat())

        # åªä¿ç•™æœ€è¿‘5æ‰¹æ¬¡çš„æ—¶é—´æˆ³
        if len(daily_data[keyword]["timestamps"]) > 5:
            daily_data[keyword]["timestamps"] = daily_data[keyword]["timestamps"][-5:]

        self.save_daily_data(date_str, daily_data)

        print(f"ğŸ“š {keyword} å½“æ—¥ç´¯ç§¯æ•°æ®: æ–°å¢ {len(new_posts)} æ¡ï¼Œç´¯è®¡ {daily_data[keyword]['total_count']} æ¡")
        return daily_data

    def get_daily_posts(self, date_str: str, keyword: str) -> List[Dict]:
        """è·å–å…³é”®è¯å½“å¤©çš„ç´¯ç§¯å¸–å­"""
        daily_data = self.load_daily_data(date_str)
        return daily_data.get(keyword, {}).get("posts", [])

    def cleanup_old_data(self, days_to_keep: int = 7):
        """æ¸…ç†è¶…è¿‡ä¸€å®šå¤©æ•°çš„æ•°æ®æ–‡ä»¶"""
        import glob
        current_date = datetime.now()
        files = glob.glob(os.path.join(self.data_dir, "daily_*.json"))

        for file_path in files:
            filename = os.path.basename(file_path)
            try:
                # æå–æ—¥æœŸéƒ¨åˆ† (daily_20251130.json -> 20251130)
                date_str = filename.replace("daily_", "").replace(".json", "")
                file_date = datetime.strptime(date_str, "%Y%m%d")

                if (current_date - file_date).days > days_to_keep:
                    os.remove(file_path)
                    print(f"ğŸ—‘ï¸ åˆ é™¤è¿‡æœŸæ¯æ—¥æ•°æ®æ–‡ä»¶: {filename}")
            except:
                continue

# ================== è¯é¢˜è®°å¿†ç®¡ç†å™¨ ==================
class TopicMemoryManager:
    def __init__(self, memory_file="./topic_memories.json"):
        self.memory_file = memory_file
        self.memories = self.load_memories()

    def load_memories(self):
        """åŠ è½½è¯é¢˜è®°å¿†"""
        if os.path.exists(self.memory_file):
            try:
                with open(self.memory_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}

    def save_memories(self):
        """ä¿å­˜è¯é¢˜è®°å¿†"""
        try:
            os.makedirs(os.path.dirname(self.memory_file) if os.path.dirname(self.memory_file) else '.', exist_ok=True)
            with open(self.memory_file, 'w', encoding='utf-8') as f:
                json.dump(self.memories, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ä¿å­˜è®°å¿†å¤±è´¥: {e}")

    def get_topic_history(self, keyword: str, limit: int = 5) -> List[str]:
        """è·å–è¯é¢˜çš„å†å²æ€»ç»“ï¼ŒæŒ‰æ—¶é—´å€’åº"""
        if keyword not in self.memories:
            return []
        history = self.memories[keyword]
        # æŒ‰æ—¶é—´æ’åºæœ€æ–°çš„åœ¨å‰é¢
        history.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
        return [item['summary'] for item in history[:limit]]

    def add_topic_summary(self, keyword: str, summary: str):
        """æ·»åŠ è¯é¢˜çš„æ–°æ€»ç»“"""
        if keyword not in self.memories:
            self.memories[keyword] = []

        timestamp = datetime.now().isoformat()
        self.memories[keyword].append({
            'timestamp': timestamp,
            'summary': summary,
            'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })

        # åªä¿ç•™æœ€è¿‘20æ¡è®°å½•
        if len(self.memories[keyword]) > 20:
            self.memories[keyword] = self.memories[keyword][-20:]

        self.save_memories()

# ================== å·¥ä½œæµç®¡ç†å™¨ ==================
class WorkflowManager:
    def __init__(self, api_key: str, prompt_folder: str, memory_path: str):
        self.api_key = api_key
        self.prompt_folder = prompt_folder
        self.memory_path = memory_path
        self.nodes = {}
        self.workflow_context = {
            "llm_client": SimpleLLMClient(api_key),
            "memory": self.load_memory(),
            "topic_memory": TopicMemoryManager(),  # æ·»åŠ è¯é¢˜è®°å¿†ç®¡ç†å™¨
            "daily_data": DailyDataCollector()  # æ·»åŠ æ¯æ—¥æ•°æ®ç´¯ç§¯å™¨
        }

    def register_node(self, config: NodeConfig):
        node_type = config.node_type
        if node_type == "receive_input":
            node = ReceiveInputNode(config, self.workflow_context)
        elif node_type == "weibo_crawl":
            node = WeiboCrawlNode(config, self.workflow_context)
        elif node_type == "llm_summarize":
            node = LLMSummarizeNode(config, self.workflow_context)
        elif node_type == "feishu_notify":
            node = FeishuNotifyNode(config, self.workflow_context)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„èŠ‚ç‚¹ç±»å‹: {node_type}")
        self.nodes[config.node_name] = node

    def run_workflow(self, inputs: Dict[str, Any], flow_config: List[Dict]):
        data_pool = {"input": inputs}
        last_output = None

        for step in flow_config:
            node_name = step["node_name"]
            node = self.nodes[node_name]

            # æ„å»ºè¾“å…¥
            input_data = {}
            for key, src in step["input_map"].items():
                source_node, output_key = src.split(".")
                input_data[key] = data_pool[source_node][output_key]

            # æ‰§è¡ŒèŠ‚ç‚¹
            print(f"\nğŸš€ æ‰§è¡ŒèŠ‚ç‚¹: {node_name}")
            try:
                output = node.execute(input_data)
                data_pool[node_name] = output
                last_output = output
            except Exception as e:
                print(f"âŒ æ‰§è¡Œå¤±è´¥: èŠ‚ç‚¹ {node_name} æ‰§è¡Œå¤±è´¥: {e}")
                return None

        return last_output

    def load_memory(self) -> Dict[str, Any]:
        if os.path.exists(self.memory_path):
            with open(self.memory_path, "r", encoding="utf-8") as f:
                return json.load(f)
        return {}


# ================== ç®€æ˜“ LLM å®¢æˆ·ç«¯ï¼ˆé€šä¹‰åƒé—®ï¼‰==================
class SimpleLLMClient:
    def __init__(self, api_key: str):
        self.api_key = api_key
        # âœ… ä½¿ç”¨é˜¿é‡Œäº‘ DashScope çš„ OpenAI å…¼å®¹æ¥å£
        self.endpoint = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
        self.model = "deepseek-r1"  # ä¹Ÿæ”¯æŒ qwen-plus, qwen-turbo ç­‰

    def send_message(self, prompt: str, json_flag: bool = False) -> str:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "Accept": "application/json"
        }

        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.7,
        }

        if json_flag:
            # å¯ç”¨ JSON è¾“å‡ºæ¨¡å¼ï¼ˆéœ€æ¨¡å‹æ”¯æŒï¼‰
            payload["response_format"] = {"type": "json_object"}

        try:
            print("ğŸ§  æ­£åœ¨è¯·æ±‚é˜¿é‡Œäº‘ Qwen...")  # è°ƒè¯•ä¿¡æ¯
            response = requests.post(
                self.endpoint,
                json=payload,
                headers=headers,
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                print("âœ… LLM è¿”å›æˆåŠŸ")
                return content
            else:
                error_msg = response.text
                print(f"âŒ LLM è¯·æ±‚å¤±è´¥: {response.status_code} {error_msg}")
                return f"[LLM é”™è¯¯] {response.status_code}: {error_msg}"

        except Exception as e:
            print(f"ğŸš¨ LLM è¯·æ±‚å¼‚å¸¸: {str(e)}")
            return f"[è¯·æ±‚å¼‚å¸¸] {str(e)}"

# ================== ä¸»ç¨‹åºå…¥å£ ==================
if __name__ == "__main__":
    # ========== âš ï¸ è¯·æ›¿æ¢ä¸ºä½ çš„é£ä¹¦ Webhookï¼ˆå¯é€‰ï¼‰==========
    FEISHU_WEBHOOK = "https://open.feishu.cn/open-apis/bot/v2/hook/77bf45ee-a658-4476-ac7e-cf3e9f538fae"  # å¯ç•™ç©ºè·³è¿‡æ¨é€

    # ========== âœ… ä½ çš„ Cookieï¼ˆå·²æ­£ç¡®åŠ è½½ï¼‰==========
    WEIBO_COOKIE = "__itrace_wid=5183d80c-8409-4bd6-aadf-b6cb3913f006; XSRF-TOKEN=n_M1OEoDnlMf_3KDZkBDs_u-; cross_origin_proto=SSL; _s_tentry=security.weibo.com; Apache=8514952670743.361.1764319483545; SINAGLOBAL=8514952670743.361.1764319483545; ULV=1764319483548:1:1:1:8514952670743.361.1764319483545:; ALF=02_1766912926; SCF=AtJxDRAJhb0kFq4S0x0diFZ5wYp67yN-uAqb1OC2du4Drb06vHPi-Sdd-pueG56Th6IHkUYoNz8tS28tXJSQXX8.; SUB=_2A25ELRbODeRhGeFN6lAW9y7EyTuIHXVnQxYGrDV8PUJbkNANLUfRkW1NQHnFhJxR7tmM7tC1woWzPSYQ_FVzKrqz; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9W5zhvkUBHraz8XPXb2gWpk.5NHD95QNe02ES0M71hzNWs4Dqcjwi--4iK.4iKnRi--ci-zEiK.7i--Ri-2RiKn7qJLf; WBPSESS=Dt2hbAUaXfkVprjyrAZT_EqNgjBcBvTShE0WQ5uUendglafG2tHDNXo7heXuRfiNnRmnpPgFQSfMCuBEch1qHc8lsGhlcV78IdZmZjWzp8l1gzq1JjECNPPzf4gQJvY9R_WYaeOSHWeYeIBzuIF4uNHoSW1Ujeer16JoAcFcylOYG6GIMr7aVlpKxhOSQAYguKyMbbbOX08ig6HmWpkjmw=="

    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆæ¨èæ–¹å¼ï¼‰
    os.environ["WEIBO_COOKIE"] = WEIBO_COOKIE
    if FEISHU_WEBHOOK:
        os.environ["FEISHU_WEBHOOK"] = FEISHU_WEBHOOK

    # åˆ›å»ºç›®å½•
    os.makedirs("./prompt", exist_ok=True)
    os.makedirs("./workflow_memory", exist_ok=True)

    # åˆå§‹åŒ–ç®¡ç†å™¨
    manager = WorkflowManager(
        api_key="sk-addb15e06fef4c19a46122a39aac8caa",  # æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ API Key
        prompt_folder="./prompt",
        memory_path="./workflow_memory/memory.json"
    )

    # æ³¨å†ŒèŠ‚ç‚¹ï¼ˆæ¨¡æ‹Ÿæµç¨‹å›¾ï¼‰
    nodes_config = [
    {
        "node_id": 1,  # â† æ”¹è¿™é‡Œï¼
        "node_type": "receive_input",
        "node_name": "receive_01",
        "input_map": {},
        "choice_map": {"default": "weibo_crawl_01"},
        "attrs": {}
    },
    {
        "node_id": 2,  # â† æ”¹è¿™é‡Œï¼
        "node_type": "weibo_crawl",
        "node_name": "weibo_crawl_01",
        "input_map": {"keyword": "receive_01.keyword"},
        "choice_map": {"default": "llm_summarize"},
        "attrs": {
            "cookie": WEIBO_COOKIE
        }
    },
    {
        "node_id": 3,  # â† æ”¹è¿™é‡Œï¼
        "node_type": "llm_summarize",
        "node_name": "llm_summarize",
        "input_map": {"posts": "weibo_crawl_01.posts", "keyword": "receive_01.keyword"},
        "choice_map": {"default": "feishu_notify"},
        "attrs": {}
    },
    {
        "node_id": 4,  # â† æ”¹è¿™é‡Œï¼
        "node_type": "feishu_notify",
        "node_name": "feishu_notify",
        "input_map": {"summary": "llm_summarize.summary"},
        "choice_map": {},
        "attrs": {
            "webhook": FEISHU_WEBHOOK
        }
    }
]

    for cfg in nodes_config:
        config = NodeConfig(**cfg)
        manager.register_node(config)

    # å®šä¹‰å·¥ä½œæµé¡ºåº
    flow = [
        {"node_name": "receive_01", "input_map": {"keyword": "input.keyword"}},
        {"node_name": "weibo_crawl_01", "input_map": {"keyword": "receive_01.keyword"}},
        {"node_name": "llm_summarize", "input_map": {"posts": "weibo_crawl_01.posts", "keyword": "receive_01.keyword"}},
        {"node_name": "feishu_notify", "input_map": {"summary": "llm_summarize.summary"}}
    ]

    # æ‰§è¡Œæµ‹è¯•
    test_inputs = {"keyword": "é»„é‡‘ä»·æ ¼ä»Šå¤©"}
    print(f"ğŸ”„ å¼€å§‹æ‰§è¡Œå·¥ä½œæµï¼Œå…³é”®è¯: {test_inputs['keyword']}")
    result = manager.run_workflow(test_inputs, flow)

    if result:
        print("\nğŸ‰ å·¥ä½œæµæ‰§è¡ŒæˆåŠŸï¼")
        if "summary" in result:
            print("\nğŸ“‹ åˆ†æç»“æœ:\n" + result["summary"])
    else:
        print("\nğŸ’¥ å·¥ä½œæµæ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
